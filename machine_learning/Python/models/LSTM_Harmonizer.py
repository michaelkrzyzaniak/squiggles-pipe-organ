import torch
import torch.nn as nn
import torch.nn.functional as F
from   torch import optim
import numpy as np
import librosa, math
import glob
import os.path
import mido
import time
import random
from mido import MidiFile
from mido import MidiTrack
from mido import Message
import soundfile


class LSTM_Harmonizer(nn.Module) :
    #-------------------------------------------------------------------------------------------
    def __init__(self, session_directory, data_directory, use_cpu=False) :
        super(LSTM_Harmonizer, self).__init__()
        
        #inclusive of both
        self.lowest_midi_note = 36
        self.highest_midi_note = 96
        
        self.sequence_length = 127
        
        self.input_size = 2048
        self.hop_size = self.input_size // 2
        self.hidden_size = 256
        self.output_size = self.highest_midi_note - self.lowest_midi_note + 1 + 1 #one extra for silence
        self.num_lstm_layers = 1
        
        self.lstm_input_size = self.hidden_size
        self.lstm_hidden_size = self.hidden_size
        
        #supposedly runs faster with batch_first = false
        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=self.lstm_hidden_size, num_layers=self.num_lstm_layers, batch_first=False)
        #self.lstm = torch.nn.RNN(input_size=self.lstm_input_size, hidden_size=self.lstm_hidden_size, num_layers=self.num_lstm_layers);
        #self.lstm = nn.LSTMCell(input_size=self.lstm_input_size, hidden_size=self.lstm_hidden_size)
        #self.fc_out = nn.Linear(self.hidden_size, self.output_size)
        #self.lstm_out_activation = torch.nn.Sigmoid()
        
        #self.lstm_out_activation = torch.nn.LogSoftmax(dim = 1)
        
        #plus 1 for counter
        self.layer_1      = torch.nn.Linear(self.input_size + self.output_size + 1, self.hidden_size);
        self.activation_1 = torch.nn.ReLU()
        self.layer_2      = torch.nn.Linear(self.lstm_hidden_size, self.output_size);
        #self.activation_2 = torch.nn.Sigmoid()
        
        #this helps these get saved and restored correctly.
        self.use_cpu                  = use_cpu
        self.sample_rate              = 44100
        self.session_directory        = session_directory
        self.data_directory           = data_directory
        self.__saved_checkpoint_batch = nn.Parameter(torch.IntTensor([0]), requires_grad=False)
        self.__sample_rate            = nn.Parameter(torch.FloatTensor([self.sample_rate]), requires_grad=False)
        self.model_save_prefix        = "model_"
        
        # display num params
        self.num_params()

    #-------------------------------------------------------------------------------------------
    def save_checkpoint(self, num_batches):
        path = os.path.join(self.session_directory, self.model_save_prefix + str(num_batches).zfill(6) + ".checkpoint")
        self.__saved_checkpoint_batch[0] = num_batches;
        torch.save(self.state_dict(), path);
        #old_checkpoints = glob.glob(os.path.join(self.session_directory, self.model_save_prefix) + "*");
        #for checkpoint in old_checkpoints:
        #    if checkpoint != path:
        #        os.remove(checkpoint);
        print("Saved " + path + " at batch " + str(num_batches));
        
    #-------------------------------------------------------------------------------------------
    def restore_if_checkpoint_exists(self, session_directory) :
        saved_model = glob.glob(os.path.join(session_directory, self.model_save_prefix) + "*.checkpoint")
        if len(saved_model) > 0:
            saved_model = sorted(saved_model)[-1];
            #todo: can't I check whether the state was loaded successfully?
            model = None
            model = torch.load(saved_model, map_location="cpu")
            
            #if torch.cuda.is_available():
            #    model = torch.load(saved_model, map_location='gpu')
            #else:
            #    model = torch.load(saved_model, map_location='cpu')

            self.load_state_dict(model)
            self.session_directory = session_directory
            #self.sample_rate = self.__sample_rate.item()
            print("Restoring checkpoint: {} pretrained with {} batches".format(saved_model, self.__saved_checkpoint_batch[0]))
        else:
            print("Creating Session: " + session_directory)

    #-------------------------------------------------------------------------------------------
    def get_saved_num_batches(self):
        return self.__saved_checkpoint_batch.item()

    #-------------------------------------------------------------------------------------------
    def calculate_audio_features(self, audio):
        n = len(audio)
        #apply hanning window
        audio = np.multiply(audio, np.hanning(n))
        
        #pad to length 2n
        audio = np.pad(audio, (0, n), 'constant', constant_values=(0, 0))
        
        #compute DFT
        spectra = np.fft.rfft(audio)
        
        #cancel noise
        #for i in range(len(spectra)) :
        #    if abs(spectra[i]) < 0.0002 : #-74 dB
        #        spectra[i]= 0+0j
  
        #square it(dft of autocorrelation)
        conjuga = np.conjugate(spectra)
        spectra = np.multiply(spectra, conjuga)
        
        #spectra = np.real(spectra)
        #compute autocorrelation
        spectra = np.fft.irfft(spectra, 2*n)
        audio = spectra[:self.input_size]
        
        #dont normalize audio, tried it, made a lot of crap in the silent sectinos
        #normalize audio
        #max = audio[0]
        #if max > 0 :
        #    audio = np.multiply(audio, 1/max)
        
        return audio
        
    
    #-------------------------------------------------------------------------------------------
    def get_active_MIDI_notes_in_time_range(self, midi, start_secs, end_secs):
        running_time = 0
        active_notes = []
        result = []
        
        for msg in midi:
            running_time += msg.time
            
            if running_time > end_secs:
                break;
    
            elif running_time < start_secs:
                if msg.type == 'note_on':
                    active_notes.append(msg);
                if msg.type == 'note_off':
                  for m in active_notes:
                      if (m.note == msg.note) and (m.channel == msg.channel):
                          active_notes.remove(m)
                  
            else: #start_secs < running_secs < end_secs
                if msg.type == 'note_on':
                    reattack = False
                    for m in active_notes:
                        if (m.note == msg.note) and (m.channel == msg.channel):
                            reattack = True
                            break;
                    if not reattack:
                        active_notes.append(msg)
    
            #if not msg.is_meta
        for m in active_notes:
            result.append(m.note);
    
        return result;

    #-------------------------------------------------------------------------------------------
    def output_notes_to_vector(self, notes):
        vector = np.zeros(self.output_size);
        last_note_index = 0
        
        #rest
        if len(notes) == 0 :
          last_note_index = self.output_size-1
          vector[last_note_index] = 1

        for note in notes:
            while note > self.highest_midi_note:
                note -= 12
            while note < self.lowest_midi_note:
                note += 12
            #++output_array[note]
            last_note_index = note-self.lowest_midi_note
            vector[last_note_index] = 1
        return vector, last_note_index

    #-------------------------------------------------------------------------------------------
    def get_random_training_filename_pair(self, is_training=True):
        training_folder = "Training" if is_training else "Validation"
        audio_directory = os.path.join(self.data_directory, training_folder, "Audio")
        midi_directory  = os.path.join(self.data_directory, training_folder, "MIDI")
        
        #FOR TOY DATA ONLY!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        input_voice_mask = random.randint(1, 14)
        #input_voice_mask = random.choice([1, 8]);
        free_voices = [];
        for i in range(4):
            if (input_voice_mask & (1 << i)) == 0:
                free_voices.append(i)
                
        output_voice_mask = 1 << (free_voices[random.randint(0, len(free_voices)-1)]);
        #output_voice_mask = 8 if (input_voice_mask == 1) else 1

        wavs = glob.glob(audio_directory + "/*_{}.wav".format(input_voice_mask));
        wav =  wavs[random.randint(0, len(wavs)-1)];
    
        mid = os.path.basename(wav)
        mid = mid.replace("_{}.wav".format(input_voice_mask), "_{}.mid".format(output_voice_mask))
        #mid = mid.replace(".wav", ".mid")
        mid = os.path.join(midi_directory, mid)
        return wav, mid
        
    #-------------------------------------------------------------------------------------------
    def get_random_training_sequence_from_file(self, wav_filename, midi_filename):
        wav, sr = librosa.load(wav_filename, sr=self.sample_rate, mono=True)
        sequence_length_in_samples = (self.sequence_length*self.hop_size-1) + self.input_size
        wav_length_in_samples = len(wav)
        if wav_length_in_samples < (wav_length_in_samples):
            print("{} could not be loaded because it is too short.".format(audio_filename))
            return None, None
        
        input_vectors = [];
        start_sample = np.random.randint(self.input_size, wav_length_in_samples-sequence_length_in_samples-1)
        for i in range(self.sequence_length):
            s = start_sample + (i*self.hop_size)
            input_vectors.append(wav[s : s+self.input_size])
            input_vectors[i] = self.calculate_audio_features(input_vectors[i])
        

        output_vectors = []
        prev_onehot_output, prev_output_note = self.output_notes_to_vector([])
        midi_file = MidiFile(midi_filename)
        start_secs = start_sample / sr
        end_secs = (start_sample + self.input_size) / sr
        hop_secs = self.hop_size / sr
        counter = 0

        for i in range(self.sequence_length):
            notes = self.get_active_MIDI_notes_in_time_range(midi_file, start_secs, end_secs)
            
            current_onehot_output, current_output_note = self.output_notes_to_vector(notes)
            output_vectors.append(current_output_note)
            input_vectors[i] = np.concatenate((input_vectors[i], prev_onehot_output, [counter]))
            prev_onehot_output = current_onehot_output;
            counter = counter + 1
            if(current_output_note != prev_output_note) :
              counter = 0
            prev_output_note = current_output_note
            start_secs += hop_secs
            end_secs += hop_secs
    
        return input_vectors, output_vectors

    #-------------------------------------------------------------------------------------------
    def get_random_training_sequence(self, is_training=True):

        wav, mid = self.get_random_training_filename_pair()
        x, y = self.get_random_training_sequence_from_file(wav, mid)
        return x, y

    #-------------------------------------------------------------------------------------------
    def get_random_training_batch(self, examples_per_batch, is_training=True):
        input_data = [];
        output_data = [];
        #training_folder = "Training" if is_training else "Validation"
        #wav_paths = glob.glob(os.path.join(self.data_directory, "Training", "Audio/*.wav"))
        
        for i in range(examples_per_batch):
            wav, mid = self.get_random_training_filename_pair()
            x, y = self.get_random_training_sequence_from_file(wav, mid)
            input_data.append(x)
            output_data.append(y)

        #LSTM wants the indices to be [sequence][batch][inputs]
        #torch.transpose(input_data , 0, 1)
        #torch.transpose(output_data, 0, 1)
        
        #jk, just output [batch][seq][inputs] because it is going into FF, and output of that will have to be separately transposed

        return input_data, output_data
    
    #-------------------------------------------------------------------------------------------
    def num_params(self) :
        parameters = filter(lambda p: p.requires_grad, self.parameters())
        parameters = sum([np.prod(p.size()) for p in parameters]) / 1000000
        print('Trainable Parameters: %.3f million' % parameters)

    #-------------------------------------------------------------------------------------------
    def time_since(self, started) :
        elapsed = time.time() - started
        m = int(elapsed // 60)
        s = int(elapsed % 60)
        if m >= 60 :
            h = int(m // 60)
            m = m % 60
            return str(h) + ":" + str(m) + ":" + str(s).zfill(2)
        else :
            return str(m) + ":" + str(s).zfill(2)
    
    #-------------------------------------------------------------------------------------------
    #https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html
    def forward(self, x, prev_state) :
    
        hidden = torch.Tensor(len(x), len(x[0]), self.lstm_input_size);
        
        if self.use_cpu == False:
          hidden  = hidden.cuda()
        
        #[batch][sequence][inputs]
        for i in range(len(x)):
          hidden[i] = self.activation_1(self.layer_1(x[i]))
        
        #[sequence][batch][inputs]
        hidden = torch.transpose(hidden , 0, 1)
        
        lstm_out, prev_state = self.lstm(hidden, prev_state)
        #lstm_out, prev_state = self.lstm(hidden.view(x.shape[0], 1, -1), prev_state)
        
        #[batch][sequence][inputs]
        lstm_out = torch.transpose(lstm_out , 0, 1)
        
        output = torch.Tensor(len(lstm_out), len(lstm_out[0]), self.output_size);
        if self.use_cpu == False:
          output  = output.cuda()
  
        for i in range(len(lstm_out)):
          output[i] = self.layer_2(lstm_out[i])

        return output, prev_state

        
        #return self.layer_2(self.activation_1(self.layer_1(x))), "ignored"

    #-------------------------------------------------------------------------------------------
    def get_initial_state(self, examples_per_batch) :
        #(cell state vector, hidden (output) state vector)
        # first argurment is num_layers
        # For LSTM
        return (torch.zeros(self.num_lstm_layers, examples_per_batch, self.lstm_hidden_size),
                torch.zeros(self.num_lstm_layers, examples_per_batch, self.lstm_hidden_size))

        # For LSTMCell
        #return (torch.zeros(sequence_length, self.lstm_hidden_size),
        #        torch.zeros(sequence_length, self.lstm_hidden_size))
        
        # For RNN
        # return torch.zeros(self.num_lstm_layers, examples_per_batch, self.lstm_hidden_size)

    #-------------------------------------------------------------------------------------------
    #https://closeheat.com/blog/pytorch-lstm-text-generation-tutorial
    def do_forward_batch_and_get_loss(self, examples_per_batch, state, is_training):
        if is_training is True:
            self.train()
        else:
            self.eval()
        
        loss_function = nn.CrossEntropyLoss()
        #loss_function = torch.nn.MSELoss()
        #loss_function = torch.nn.BCELoss()
        #loss_function = torch.nn.NLLLoss()
        input, target_output = self.get_random_training_batch(examples_per_batch, is_training)
        #input, target_output = self.get_random_training_sequence(is_training)
        
        input = torch.FloatTensor(input)
        #target_output = torch.FloatTensor(target_output)
        target_output = torch.LongTensor(target_output) #For CrossEntropyLoss  Loss
        
        
        if self.use_cpu == False:
            input  = input.cuda()
            target_output = target_output.cuda()

        output, state = self(input, state)
        
        
        loss = 0;
        for i in range(len(output)):
          loss += loss_function(output[i], target_output[i])
          
        return loss, state
    
    #-------------------------------------------------------------------------------------------
    #https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html
    def train_model(self, num_batches, examples_per_batch, save_every, lr) :
        optimizer = optim.Adam(self.parameters())
        #optimizer = optim.SGD(self.parameters(), lr, momentum=0.9)
        for p in optimizer.param_groups : p['lr'] = lr
        start = time.time()
        
        #todo: how often to init state? Tutorial has once per epoch...
        #loss_function = torch.nn.BCELoss()
        
        for batch in range(self.get_saved_num_batches(), num_batches) :
            optimizer.zero_grad()
            state = self.get_initial_state(examples_per_batch)
            
            loss, state = self.do_forward_batch_and_get_loss(examples_per_batch, state, True)
            loss.backward()
            #torch.nn.utils.clip_grad_norm_(self.parameters(), 1)
            optimizer.step()
            
            elapsed = self.time_since(start)
            speed = (time.time() - start) / (batch + 1)

            print("Batch {0} of {1} --- Training Loss: {2} --- Elapsed Time: {3} --- Sec / Batch: {4}".format(batch + 1, num_batches, loss.item(), elapsed, speed))
            if (((batch+1) % save_every) == 0) or (batch is num_batches-1):
                self.save_checkpoint(batch+1)
                self.sample();


    #-------------------------------------------------------------------------------------------
    def sample(self):
        #self.reverse_synthesize_gold_standard("Bach_Minuet")
        #self.reverse_synthesize_gold_standard("Fugue")
        #self.reverse_synthesize_gold_standard("Bach_Saraband")
        #self.reverse_synthesize_gold_standard("MIDI_1")
        #self.reverse_synthesize_gold_standard("MIDI_2")
        #self.reverse_synthesize_gold_standard("Bass")
        #self.reverse_synthesize_gold_standard("Soprano")
        #self.reverse_synthesize_gold_standard("All_But_Alto")
        state = self.get_initial_state(1);
        #input = np.random.rand(self.input_size + self.output_size + 1)
        input = [0.9131681, 0.56954217, 0.693143, 0.27987552, 0.9384719, 0.5332751, 0.033197656, 0.260515, 0.9891609, 0.74611115, 0.8151036, 0.62090844, 0.43094224, 0.08054225, 0.8370375, 0.38907412, 0.77601725, 0.9567666, 0.59922713, 0.22599801, 0.19051546, 0.37057742, 0.77277744, 0.1939651, 0.6233447, 0.40411744, 0.6546862, 0.6232228, 0.8230969, 0.98731697, 0.8907481, 0.6405171, 0.49696267, 0.3894121, 0.8615779, 0.99796337, 0.19872124, 0.44871545, 0.54791814, 0.5283414, 0.48364413, 0.05144732, 0.6045274, 0.8058979, 0.41906545, 0.6501374, 0.71623176, 0.56522405, 0.57856554, 0.9335452, 0.4426715, 0.3738298, 0.18237706, 0.40224963, 0.8722422, 0.31967366, 0.43200225, 0.7480112, 0.4067554, 0.5983035, 0.77089983, 0.49811137, 0.50941396, 0.78123283, 0.18746649, 0.27759174, 0.65854347, 0.31128046, 0.84709585, 0.9597482, 0.5146036, 0.4079393, 0.9445861, 0.6017181, 0.32751384, 0.47805876, 0.6452384, 0.1401353, 0.51832473, 0.034800004, 0.9120051, 0.29830828, 0.062253322, 0.81201804, 0.8100589, 0.47989047, 0.87312806, 0.71032864, 0.75536454, 0.13725445, 0.6859512, 0.015874662, 0.23478599, 0.86557245, 0.8581595, 0.7251877, 0.23663324, 0.13690645, 0.8676262, 0.46013525, 0.5921732, 0.22411008, 0.42997038, 0.739181, 0.95094216, 0.12498269, 0.25261125, 0.16080156, 0.6382261, 0.6493578, 0.19854148, 0.46935058, 0.96510446, 0.49223286, 0.9078433, 0.8563022, 0.071159795, 0.95194346, 0.35597026, 0.78390163, 0.42654738, 0.59760725, 0.34552968, 0.23236674, 0.5549285, 0.6934422, 0.7230942, 0.2897222, 0.03880484, 0.14086254, 0.90476745, 0.70117456, 0.4202743, 0.909399, 0.25342187, 0.2829066, 0.7792072, 0.9971245, 0.30648047, 0.16014153, 0.76117444, 0.5587081, 0.92960465, 0.008406889, 0.13314915, 0.9128669, 0.699841, 0.019217608, 0.50522435, 0.8020802, 0.6641795, 0.27520508, 0.3616417, 0.122658424, 0.5596082, 0.6189746, 0.71887755, 0.12822531, 0.72362953, 0.92203367, 0.6047021, 0.5843974, 0.2371139, 0.992849, 0.24140501, 0.15700836, 0.19292694, 0.022321232, 0.786145, 0.10845767, 0.38324922, 0.007010676, 0.47966456, 0.55849767, 0.19592142, 0.3493293, 0.72813624, 0.7009098, 0.43627384, 0.39634362, 0.5762477, 0.14058481, 0.2540135, 0.38634232, 0.48797923, 0.725176, 0.6998704, 0.39167637, 0.31491926, 0.4136053, 0.7849033, 0.1339958, 0.5642448, 0.14242674, 0.3711069, 0.99850804, 0.2663984, 0.4166808, 0.1757622, 0.44607732, 0.05517104, 0.13399194, 0.2755801, 0.98368865, 0.6420358, 0.72989994, 0.17102358, 0.91375077, 0.400251, 0.64243406, 0.99585956, 0.32409167, 0.84152305, 0.48796237, 0.028324207, 0.66543686, 0.6523075, 0.28825492, 0.50766844, 0.9167577, 0.057177372, 0.90910757, 0.7921431, 0.53672355, 0.5933532, 0.6956034, 0.37069008, 0.12558837, 0.6825735, 0.011440807, 0.9833257, 0.94667226, 0.83108974, 0.22716022, 0.118295535, 0.33482575, 0.7560334, 0.7248675, 0.954379, 0.85105217, 0.3363719, 0.5271458, 0.149218, 0.6532333, 0.18186624, 0.63196814, 0.4392246, 0.58514804, 0.17097771, 0.33709976, 0.49083462, 0.6650753, 0.27746648, 0.993248, 0.43501568, 0.65744346, 0.7493093, 0.062116228, 0.10020765, 0.79402405, 0.06865461, 0.43514797, 0.010636628, 0.6587839, 0.32629067, 0.5337112, 0.93664455, 0.87319905, 0.07102111, 0.37129012, 0.4933488, 0.2294816, 0.243957, 0.12555844, 0.0794053, 0.05842346, 0.17950928, 0.53237885, 0.542283, 0.25792724, 0.0062900176, 0.2359227, 0.25855228, 0.58414865, 0.49205062, 0.688139, 0.6593756, 0.3080755, 0.07554827, 0.54987824, 0.5226976, 0.7462346, 0.7860864, 0.66722, 0.69466, 0.89325666, 0.9070155, 0.9649255, 0.5521734, 0.49714363, 0.6078322, 0.6449101, 0.31925526, 0.7545385, 0.105826914, 0.79729325, 0.14544484, 0.57453114, 0.400902, 0.79131454, 0.062298793, 0.12728453, 0.9045092, 0.3768444, 0.78706574, 0.030356234, 0.659217, 0.25785783, 0.2740398, 0.89150625, 0.2497035, 0.67275625, 0.22041026, 0.5121298, 0.61484, 0.342017, 0.20834978, 0.6467658, 0.17600438, 0.5598752, 0.6283088, 0.47290358, 0.16759835, 0.1542983, 0.90713435, 0.6915755, 0.60022587, 0.28038737, 0.9616888, 0.47069213, 0.15792975, 0.0624658, 0.5887893, 0.4509325, 0.9785684, 0.23981191, 0.94391906, 0.34260324, 0.29844192, 0.98066825, 0.14619304, 0.22664763, 0.67959976, 0.92221624, 0.6010556, 0.55830526, 0.8145663, 0.4533591, 0.35073256, 0.078548394, 0.18290833, 0.79555225, 0.09160035, 0.8744133, 0.55117816, 0.85063607, 0.5033785, 0.94791937, 0.5237435, 0.6637835, 0.60729975, 0.6808722, 0.78502345, 0.55035466, 0.19551826, 0.0703299, 0.5531608, 0.49537897, 0.43880582, 0.6926091, 0.66652435, 0.10400564, 0.58238494, 0.12449729, 0.108819455, 0.44026184, 0.3589051, 0.53526944, 0.82861185, 0.20608096, 0.054823846, 0.4910291, 0.22806297, 0.99251485, 0.76053166, 0.06944966, 0.014438241, 0.9066966, 0.24137539, 0.31535757, 0.32828698, 0.40805662, 0.69388765, 0.27725512, 0.40978974, 0.45099515, 0.916421, 0.39472666, 0.9373244, 0.8182402, 0.4466866, 0.041951, 0.8923397, 0.09232949, 0.3027349, 0.50229144, 0.124188945, 0.69350255, 0.39488697, 0.97622186, 0.8257318, 0.75406593, 0.3904474, 0.6237429, 0.48796204, 0.05324292, 0.847943, 0.32419333, 0.1277868, 0.28994673, 0.5068754, 0.94557655, 0.16247162, 0.84678775, 0.9785898, 0.97435296, 0.06692638, 0.1058549, 0.171835, 0.73247945, 0.43526322, 0.8955568, 0.6531289, 0.34770852, 0.853743, 0.2093798, 0.7216953, 0.915952, 0.37455732, 0.64120924, 0.80012584, 0.7242407, 0.25294203, 0.20768684, 0.6595563, 0.5847335, 0.52022386, 0.574696, 0.34282598, 0.48690936, 0.27313128, 0.848991, 0.90423465, 0.15031047, 0.15910971, 0.5469843, 0.25092813, 0.75901335, 0.5927635, 0.6882917, 0.5601733, 0.5938114, 0.5907808, 0.01886084, 0.611722, 0.7583899, 0.8117202, 0.7567537, 0.3395038, 0.06486046, 0.62544596, 0.7688556, 0.5025398, 0.06736005, 0.43886063, 0.37213132, 0.17821385, 0.97821313, 0.8080782, 0.85569644, 0.1714974, 0.16865848, 0.53915757, 0.63776046, 0.16478698, 0.08139107, 0.8515487, 0.755356, 0.0062813372, 0.9936396, 0.8040489, 0.82777476, 0.122591026, 0.10544787, 0.11883393, 0.7851376, 0.46265933, 0.70027965, 0.6023116, 0.8635646, 0.6749149, 0.3697405, 0.31184494, 0.6277818, 0.5874012, 0.15151195, 0.6552619, 0.012287048, 0.8242309, 0.045788895, 0.8625709, 0.93355274, 0.7820996, 0.39749733, 0.19282235, 0.6999079, 0.21054704, 0.33928755, 0.32426926, 0.6618299, 0.004639455, 0.121084854, 0.9567653, 0.36209548, 0.99953055, 0.39417675, 0.20778392, 0.813372, 0.27233317, 0.8262919, 0.10081369, 0.93286335, 0.62821436, 0.22288863, 0.019662332, 0.2769, 0.6901597, 0.3602852, 0.40268055, 0.81669337, 0.76507527, 0.6597915, 0.24129592, 0.45903975, 0.08149456, 0.9210143, 0.8315381, 0.43050125, 0.6802355, 0.06144917, 0.59900546, 0.22868928, 0.75543463, 0.35201302, 0.20065758, 0.11864324, 0.8746064, 0.36030298, 0.32060418, 0.9498133, 0.5246602, 0.8577219, 0.6836373, 0.020169387, 0.14901534, 0.8050542, 0.94837666, 0.36756462, 0.78717166, 0.21731283, 0.64423496, 0.8864693, 0.29003823, 0.10417043, 0.38806975, 0.583797, 0.022335604, 0.2105729, 0.2119277, 0.6915785, 0.63752854, 0.9367936, 0.7443616, 0.713302, 0.015600102, 0.5453251, 0.71045274, 0.5280967, 0.61799467, 0.48354435, 0.26896265, 0.25322974, 0.25671968, 0.22140108, 0.7079325, 0.19126733, 0.008945714, 0.91455185, 0.05962459, 0.41447714, 0.9245913, 0.11052128, 0.97668236, 0.8782275, 0.16746205, 0.5538914, 0.43049937, 0.41942775, 0.26624295, 0.44805017, 0.8282905, 0.91898656, 0.8781801, 0.8869478, 0.6475714, 0.94340944, 0.85600746, 0.024367547, 0.07088649, 0.40708324, 0.78139925, 0.32201555, 0.6043018, 0.93580574, 0.62447894, 0.6884222, 0.86768967, 0.87357605, 0.51980567, 0.3216508, 0.5915586, 0.7355165, 0.8911848, 0.9562269, 0.008385958, 0.93785113, 0.34994048, 0.022089222, 0.8797031, 0.4503058, 0.89489996, 0.79254794, 0.18248563, 0.0253584, 0.22049949, 0.4071508, 0.82857436, 0.74837947, 0.7677953, 0.9868205, 0.94552386, 0.16079272, 0.9352748, 0.15029265, 0.541326, 0.18399331, 0.03489992, 0.66645664, 0.7694615, 0.47486773, 0.63017607, 0.9103025, 0.58518195, 0.43869442, 0.49060693, 0.6677939, 0.7542828, 0.53591657, 0.4171021, 0.5473645, 0.7060122, 0.8502921, 0.87451684, 0.18307737, 0.7351954, 0.38677272, 0.49963963, 0.8698299, 0.63246757, 0.39515495, 0.8339204, 0.12961526, 0.51866055, 0.13062003, 0.9057483, 0.7784966, 0.9584447, 0.13805544, 0.5555896, 0.03310422, 0.9576051, 0.28787246, 0.1534353, 0.18286677, 0.36391875, 0.9365114, 0.75813776, 0.80909234, 0.7928806, 0.7686298, 0.50734687, 0.1463356, 0.6125735, 0.38808665, 0.8531455, 0.42293844, 0.93276787, 0.91118073, 0.062420134, 0.744122, 0.19998841, 0.2034427, 0.49364108, 0.51175255, 0.7929438, 0.4192747, 0.11937324, 0.4746808, 0.74343693, 0.13864866, 0.45942223, 0.2885073, 0.14742558, 0.17715304, 0.44125354, 0.38117486, 0.6130171, 0.7991505, 0.6532841, 0.8945403, 0.5163174, 0.89011884, 0.011360482, 0.7470389, 0.21248692, 0.015294264, 0.4204874, 0.3820602, 0.07906616, 0.5329247, 0.35495734, 0.35503104, 0.4575183, 0.01245144, 0.9644506, 0.7167577, 0.36958793, 0.5814156, 0.6186523, 0.3126716, 0.4589687, 0.88512427, 0.4713633, 0.8107364, 0.57656306, 0.010297218, 0.060456168, 0.7304054, 0.022753404, 0.9140327, 0.9268466, 0.79673654, 0.38643104, 0.21276166, 0.6243157, 0.068270124, 0.8957702, 0.17141356, 0.19321352, 0.16698141, 0.3949354, 0.0016545326, 0.25155476, 0.28344575, 0.6220575, 0.42751804, 0.09029681, 0.56919163, 0.7987288, 0.7601294, 0.3272729, 0.75268996, 0.35202807, 0.30008614, 0.9049976, 0.24147893, 0.598936, 0.69006497, 0.13677908, 0.96356344, 0.46580112, 0.6573878, 0.5682397, 0.39701974, 0.86281985, 0.59310263, 0.73343617, 0.3565937, 0.47379074, 0.947634, 0.47931743, 0.07849753, 0.22460246, 0.29307306, 0.07657475, 0.83651114, 0.15232717, 0.25954962, 0.91276896, 0.2050007, 0.9059576, 0.92557544, 0.53490734, 0.009002205, 0.7023752, 0.31565502, 0.64520866, 0.49359128, 0.47162014, 0.33355322, 0.5793333, 0.104870655, 0.25359148, 0.5942389, 0.13082635, 0.44950655, 0.9982699, 0.34144488, 0.43157586, 0.15383229, 0.60653687, 0.66345763, 0.3527884, 0.4542219, 0.5057096, 0.67702097, 0.50859827, 0.54339486, 0.56183183, 0.69662625, 0.43285528, 0.20015414, 0.27704373, 0.14453878, 0.4455134, 0.5481672, 0.43124834, 0.5503988, 0.51821893, 0.012771052, 0.9408617, 0.20976578, 0.9647768, 0.7162613, 0.2157905, 0.82388663, 0.26531824, 0.32813457, 0.012890794, 0.95692295, 0.5584451, 0.6390036, 0.005720725, 0.883058, 0.6588494, 0.2495388, 0.6192949, 0.4611064, 0.62705517, 0.8082642, 0.48047066, 0.14184523, 0.5599125, 0.91369224, 0.3491292, 0.42821527, 0.8689083, 0.8442221, 0.02960056, 0.12291209, 0.2478016, 0.096445315, 0.61022854, 0.118536755, 0.5165015, 0.38527247, 0.03674785, 0.57788473, 0.4269273, 0.2838761, 0.8489011, 0.9936423, 0.2150738, 0.87471986, 0.48456016, 0.6051259, 0.48148754, 0.77500886, 0.21253562, 0.71012044, 0.53561515, 0.11319264, 0.75164974, 0.076123394, 0.7441353, 0.28249672, 0.67289084, 0.8268405, 0.13164508, 0.45993158, 0.6558098, 0.156203, 0.14147344, 0.0988342, 0.44534662, 0.17068961, 0.42625153, 0.555127, 0.006894911, 0.31479988, 0.53204703, 0.5230421, 0.1264836, 0.7827898, 0.7200373, 0.9331454, 0.48702538, 0.7235288, 0.1877636, 0.5543824, 0.7054289, 0.6433667, 0.7215565, 0.14668167, 0.94552684, 0.66571105, 0.4916845, 0.7310999, 0.4027991, 0.38474002, 0.56767654, 0.23880035, 0.9936389, 0.7511554, 0.6350039, 0.83929604, 0.65147054, 0.95752925, 0.7812308, 0.021845017, 0.96165496, 0.22781307, 0.23279527, 0.47247487, 0.10133966, 0.07074117, 0.0027464465, 0.93606824, 0.81143755, 0.9786663, 0.9309136, 0.6176547, 0.85899705, 0.93963784, 0.09276249, 0.788979, 0.3326749, 0.67623836, 0.25871414, 0.281204, 0.30212903, 0.41046736, 0.39694166, 0.2162259, 0.49620858, 0.2439861, 0.50072855, 0.46028388, 0.60015893, 0.4855364, 0.980436, 0.4200478, 0.9228794, 0.17560573, 0.6913907, 0.44230512, 0.54675466, 0.5461427, 0.3533747, 0.56744576, 0.07629887, 0.8556519, 0.23812155, 0.4744814, 0.48215133, 0.968789, 0.95070267, 0.06876927, 0.055701442, 0.28996179, 0.59772575, 0.41532072, 0.9563995, 0.42632762, 0.45969263, 0.8549836, 0.13883954, 0.35691747, 0.6316159, 0.16918983, 0.012686323, 0.7374087, 0.008923607, 0.5651115, 0.14937867, 0.90168303, 0.835598, 0.20234445, 0.6475502, 0.19592924, 0.3628268, 0.59646374, 0.38659245, 0.85969156, 0.42095417, 0.22249775, 0.048354305, 0.3679326, 0.2727878, 0.40352193, 0.04975278, 0.27656105, 0.5542092, 0.7659761, 0.111883, 0.241574, 0.24660876, 0.82849663, 0.4211802, 0.38027096, 0.6896407, 0.8092955, 0.17457934, 0.34144545, 0.7818308, 0.1469365, 0.93534094, 0.76541704, 0.6670946, 0.62490046, 0.27971613, 0.31246588, 0.91980433, 0.89522207, 0.69431084, 0.70439875, 0.40396968, 0.4524195, 0.9072761, 0.66813946, 0.55635756, 0.21423295, 0.55684483, 0.8462668, 0.8039361, 0.11278839, 0.82534254, 0.8815589, 0.84115547, 0.56300336, 0.5064584, 0.2697475, 0.2078977, 0.4918288, 0.6420165, 0.03111195, 0.11443899, 0.28213316, 0.1070999, 0.17244351, 0.4743062, 0.53382844, 0.97665924, 0.1310395, 0.28698543, 0.042828754, 0.35214305, 0.74397534, 0.1384948, 0.69559777, 0.04067348, 0.87634087, 0.13130505, 0.6924523, 0.25687766, 0.38197765, 0.5301134, 0.29580036, 0.45612487, 0.77385706, 0.98759717, 0.89499587, 0.3799969, 0.6897362, 0.86104125, 0.09877389, 0.2452947, 0.04045362, 0.100871354, 0.19345152, 0.6571206, 0.9067888, 0.0815335, 0.035389494, 0.0711849, 0.49228373, 0.4564307, 0.9484811, 0.58903795, 0.15170966, 0.21046405, 0.17050065, 0.33101386, 0.6752732, 0.09412192, 0.5241478, 0.6236873, 0.2489938, 0.09488578, 0.14932604, 0.8541673, 0.05947388, 0.47936013, 0.32953873, 0.7141885, 0.13179022, 0.7884747, 0.9171149, 0.65204334, 0.66844106, 0.30086228, 0.9474112, 0.56581295, 0.35397753, 0.09745628, 0.11790072, 0.9947402, 0.66799814, 0.894907, 0.13798414, 0.88699704, 0.6320226, 0.5671418, 0.9714316, 0.88647765, 0.31099662, 0.9350102, 0.14633022, 0.064937524, 0.38654467, 0.6801332, 0.6866543, 0.9888037, 0.9764609, 0.24654989, 0.4261351, 0.6447412, 0.16021991, 0.6382491, 0.9463491, 0.50670135, 0.3767857, 0.24075152, 0.9221541, 0.7823315, 0.09825139, 0.6311297, 0.39483565, 0.6199956, 0.21602905, 0.003466184, 0.1795921, 0.62704647, 0.82597446, 0.21823713, 0.7412322, 0.8545819, 0.06828104, 0.18483722, 0.2400315, 0.65447915, 0.24475275, 0.48634127, 0.65773, 0.99126095, 0.81819564, 0.43982416, 0.7763108, 0.14262919, 0.1115694, 0.018581903, 0.4799216, 0.7011298, 0.9816392, 0.66683835, 0.2820086, 0.17371616, 0.0923229, 0.2456096, 0.38289234, 0.83604836, 0.3993178, 0.06614715, 0.6978978, 0.29235134, 0.868014, 0.8852317, 0.43973702, 0.7820457, 0.725559, 0.45200047, 0.19282435, 0.58715194, 0.70386064, 0.7428732, 0.098584495, 0.5146202, 0.6461855, 0.5481674, 0.77747416, 0.122035936, 0.83937156, 0.8136016, 0.6457048, 0.6574128, 0.68152004, 0.6986535, 0.0783156, 0.16405347, 0.23141572, 0.6097845, 0.12465079, 0.94537824, 0.9282058, 0.092798926, 0.5379392, 0.13259175, 0.9558888, 0.7058725, 0.57169324, 0.6998346, 0.9661542, 0.28368744, 0.73558813, 0.78487074, 0.609755, 0.73638964, 0.60874313, 0.9142091, 0.68887866, 0.71675086, 0.058090232, 0.16753435, 0.6644243, 0.06014828, 0.34387854, 0.9288771, 0.8654691, 0.5263701, 0.5210464, 0.23536392, 0.7316288, 0.41648418, 0.57421714, 0.40435487, 0.5871536, 0.061702248, 0.5347481, 0.6400173, 0.8375817, 0.07271751, 0.1646187, 0.33708498, 0.51452863, 0.38245326, 0.84690475, 0.8227614, 0.507484, 0.44550088, 0.37869495, 0.3062055, 0.7794081, 0.20759402, 0.4686894, 0.454317, 0.13170679, 0.609939, 0.32796252, 0.52760565, 0.44287214, 0.8633769, 0.2706109, 0.22405644, 0.96218884, 0.16751736, 0.024281815, 0.9851754, 0.92409897, 0.36594203, 0.31653756, 0.9952359, 0.9958543, 0.27064836, 0.5063529, 0.24457264, 0.9989191, 0.9340197, 0.46866918, 0.55760473, 0.08434612, 0.39274648, 0.93210614, 0.3277281, 0.39489186, 0.5082841, 0.944047, 0.11698131, 0.7397088, 0.23920785, 0.31835315, 0.49002355, 0.42824075, 0.044365708, 0.04423369, 0.38815033, 0.13127469, 0.9164914, 0.4974702, 0.14572315, 0.4618181, 0.03835926, 0.36313266, 0.99056536, 0.48706597, 0.5073671, 0.3950117, 0.7045956, 0.37889254, 0.026582617, 0.2157355, 0.2144333, 0.72511756, 0.39642432, 0.59923935, 0.9805346, 0.9106737, 0.6905617, 0.12239444, 0.3341657, 0.26154336, 0.9812079, 0.78865784, 0.6596731, 0.33810487, 0.92685467, 0.23015246, 0.5777659, 0.33161262, 0.49725994, 0.6478783, 0.34231487, 0.07229273, 0.7462695, 0.87512153, 0.70371425, 0.05110172, 0.012592116, 0.37228248, 0.6427244, 0.8754374, 0.7425083, 0.2698107, 0.20443562, 0.6327704, 0.10112409, 0.5609742, 0.4639013, 0.89573586, 0.6778047, 0.013322537, 0.036320996, 0.4174824, 0.57273203, 0.9307835, 0.6268126, 0.7361411, 0.9231694, 0.81110257, 0.49203032, 0.1109932, 0.26033843, 0.37173244, 0.21759292, 0.11541878, 0.6347407, 0.76290464, 0.45670187, 0.64831424, 0.22086513, 0.3103456, 0.4739363, 0.8844554, 0.4139253, 0.93503666, 0.6755726, 0.13677505, 0.23458758, 0.12610185, 0.094299056, 0.19408882, 0.12546511, 0.7154582, 0.8169112, 0.25502992, 0.08396492, 0.70345336, 0.68082494, 0.6678526, 0.019146215, 0.55611587, 0.85516375, 0.87740135, 0.13064198, 0.0141342385, 0.47366497, 0.42399135, 0.296277, 0.47867647, 0.7985021, 0.6017287, 0.09354998, 0.66373444, 0.5636881, 0.37644532, 0.66272765, 0.3165485, 0.8765899, 0.87860143, 0.12806022, 0.95012593, 0.6789444, 0.5531825, 0.60322714, 0.36840183, 0.5731153, 0.70717525, 0.38649154, 0.5045931, 0.2941737, 0.6854268, 0.5554547, 0.6960864, 0.831052, 0.44209516, 0.6345573, 0.080718465, 0.47382477, 0.2984647, 0.71868944, 0.014975271, 0.20190027, 0.2184572, 0.20860581, 0.0031482968, 0.7206103, 0.4473431, 0.32485506, 0.5247934, 0.94461393, 0.6470409, 0.63119817, 0.1263697, 0.54541904, 0.14390066, 0.1706287, 0.53486633, 0.81370336, 0.25797778, 0.7940683, 0.8856466, 0.51850003, 0.9779458, 0.6307537, 0.52344733, 0.13381143, 0.49784645, 0.32504418, 0.0814465, 0.43903142, 0.14908609, 0.88457584, 0.79048264, 0.16541037, 0.04622655, 0.7058357, 0.74135685, 0.29323554, 0.21859062, 0.8581714, 0.18959385, 0.88767105, 0.91020966, 0.38229537, 0.18504623, 0.60124576, 0.5528019, 0.27831006, 0.8815117, 0.65340376, 0.7423467, 0.5843674, 0.018355656, 0.25887033, 0.17649926, 0.68269974, 0.096234426, 0.8141229, 0.6092282, 0.07490408, 0.29692066, 0.70685047, 0.06546976, 0.99552536, 0.26903164, 0.23556536, 0.10593123, 0.7018155, 0.636364, 0.44334733, 0.7536734, 0.81042707, 0.024569409, 0.6934057, 0.3346872, 0.029692115, 0.76546735, 0.97087806, 0.42157376, 0.62680644, 0.268388, 0.47596595, 0.10486793, 0.83886856, 0.4185003, 0.11446048, 0.0023005644, 0.44093692, 0.552792, 0.13392158, 0.18578735, 0.38990715, 0.4621072, 0.05211224, 0.02591123, 0.1671175, 0.68067896, 0.24174224, 0.069290385, 0.8651579, 0.26898792, 0.8323754, 0.3799517, 0.891029, 0.120709166, 0.58503884, 0.58879703, 0.62297845, 0.22788641, 0.9346141, 0.73052746, 0.38639864, 0.2915305, 0.63633883, 0.30090725, 0.7973354, 0.52702075, 0.9050213, 0.29187036, 0.3671988, 0.047923163, 0.053228788, 0.65279835, 0.34973514, 0.014377639, 0.55470926, 0.6746403, 0.13351622, 0.55190617, 0.5842762, 0.26757157, 0.199952, 0.85281116, 0.86574763, 0.4391646, 0.05836034, 0.766563, 0.06247865, 0.5731156, 0.89956707, 0.6734207, 0.4062671, 0.675526, 0.74116904, 0.22209494, 0.2846604, 0.42126107, 0.83583087, 0.6471389, 0.48485994, 0.6752822, 0.63268214, 0.13765645, 0.20215157, 0.22694086, 0.6140733, 0.95154405, 0.23675151, 0.1622124, 0.7911897, 0.015690614, 0.9239585, 0.69631153, 0.8314919, 0.91876733, 0.98866004, 0.1542163, 0.120904505, 0.8521038, 0.025367625, 0.7297573, 0.96927595, 0.7286933, 0.96374315, 0.6051681, 0.08604529, 0.33714303, 0.5384371, 0.44629297, 0.5193554, 0.21260902, 0.29177508, 0.4077251, 0.1824936, 0.43872583, 0.06703859, 0.6639534, 0.28217286, 0.7853937, 0.24687448, 0.22599378, 0.53779924, 0.85869575, 0.2722161, 0.15218876, 0.5543802, 0.68491334, 0.04057311, 0.6532562, 0.43496314, 0.7644107, 0.037222844, 0.11121372, 0.33360332, 0.15397425, 0.20529354, 0.6345938, 0.46953222, 0.37557536, 0.8696789, 0.15590736, 0.8891655, 0.65705824, 0.9553438, 0.14381948, 0.12451958, 0.7538074, 0.54679614, 0.8790241, 0.5004133, 0.45738396, 0.6838263, 0.642905, 0.670322, 0.17877345, 0.91653246, 0.637355, 0.5703828, 0.8347907, 0.528863, 0.97064924, 0.8070417, 0.6876849, 0.33925396, 0.17015027, 0.20833988, 0.070324816, 0.15642838, 0.28868803, 0.9382798, 0.17210352, 0.808014, 0.58359146, 0.6107069, 0.5225599, 0.50186276, 0.004462431, 0.29732823, 0.98925954, 0.8127511, 0.57559144, 0.71842307, 0.25523016, 0.60104257, 0.8135401, 0.6696782, 0.1442953, 0.8576508, 0.14496574, 0.53848016, 0.41993922, 0.392815, 0.3619257, 0.22691543, 0.2553007, 0.17125095, 0.6265236, 0.61222917, 0.25661194, 0.61667675, 0.52690613, 0.080522016, 0.17493147, 0.8800111, 0.61994326, 0.6492831, 0.4755794, 0.6658453, 0.08784841, 0.84881884, 0.17327477, 0.8583435, 0.29015392, 0.14311251, 0.24066678, 0.42950636, 0.25080085, 0.961727, 0.2889193, 0.7193492, 0.5619685, 0.75539976, 0.46017545, 0.45698434, 0.2546062, 0.8377219, 0.21810025, 0.33331057, 0.30213472, 0.828155, 0.33290067, 0.7005061, 0.013833042, 0.22925398, 0.86172736, 0.26184607, 0.3444603, 0.102931745, 0.0043939753, 0.87564397, 0.68933696, 0.58461046, 0.2882523, 0.9029048, 0.14638402, 0.9493173, 0.015440651, 0.5080176, 0.5806808, 0.5780222, 0.5266751, 0.19069073, 0.7172896, 0.81849474, 0.07739408, 0.21003872, 0.00823891, 0.26520848, 0.9097745, 0.31092176, 0.5201311, 0.30503002, 0.72749406, 0.77363354, 0.7032288, 0.8918848, 0.57912225, 0.825056, 0.9694121, 0.51618993, 0.35328102, 0.6699309, 0.16414407, 0.36302963, 0.84171075, 0.08572056, 0.17996262, 0.8700738, 0.70429766, 0.8008164, 0.6224444, 0.34802735, 0.44152516, 0.49099588, 0.46147364, 0.49446896, 0.50541365, 0.47703543, 0.4028351, 0.6634854, 0.8749369, 0.71689874, 0.5573511, 0.6319725, 0.65356374, 0.7037742, 0.442561, 0.8960011, 0.050937314, 0.25036573, 0.50824547, 0.7988776, 0.5040195, 0.568732, 0.8485795, 0.6700038, 0.96485966, 0.83524364, 0.905007, 0.75968, 0.56315655, 0.3018047, 0.5951851, 0.21533193, 0.0040783724, 0.35849795, 0.035321124, 0.16085386, 0.49507007, 0.22863509, 0.81598455, 0.84421974, 0.62012136, 0.58906686, 0.39085278, 0.20252675, 0.52057725, 0.74778664, 0.818212, 0.9568817, 0.48818377, 0.41537055, 0.31529355, 0.50608784, 0.30680662, 0.33090574, 0.32512537, 0.9777072, 0.2701814, 0.7768436, 0.52228576, 0.12165337, 0.94995457, 0.3295511, 0.39794216, 0.5868999, 0.8320266, 0.20533927, 0.469799, 0.26632115, 0.3443881, 0.67738926, 0.24766749, 0.46492505, 0.7593105, 0.43581623, 0.045094267, 0.0451162, 0.84841853, 0.1906694, 0.3562037, 0.7352471, 0.07308672, 0.021955756, 0.76479775, 0.8427103, 0.43610358, 0.64465255, 0.8436477, 0.43484783, 0.7487168, 0.08247913, 0.9856132, 0.5535974, 0.6795124, 0.72459334, 0.9543029, 0.4983789, 0.2634089, 0.8926405, 0.059553858, 0.95986474, 0.70653635, 0.25922465, 0.23040982, 0.79307395, 0.09393381, 0.95262843, 0.96933156, 0.8553438, 0.37392864, 0.8630658, 0.6889179, 0.86754405, 0.3718383, 0.5020844, 0.52966714, 0.3644047, 0.71463096, 0.5963779, 0.34370437, 0.5789733, 0.8796625, 0.9696267, 0.65023214, 0.8884274, 0.83233005, 0.17127582, 0.9173948, 0.5447069, 0.4867675, 0.18614116, 0.76850384, 0.53519136, 0.06659559, 0.4561151, 0.074284025, 0.767517, 0.94183147, 0.019119091, 0.24579982, 0.23222923, 0.5339148, 0.7931428, 0.4645768, 0.5930503, 0.23545969, 0.69075924, 0.19271082, 0.9577473, 0.26400867, 0.34572893, 0.38911474, 0.8899522, 0.120754026, 0.57764184, 0.015694251, 0.3325355, 0.51933545, 0.073832564, 0.037177008, 0.558405, 0.44452673, 0.34773013, 0.4099467, 0.53643674, 0.6963272, 0.65943825, 0.6563482, 0.37879276, 0.7151241, 0.48170087, 0.018570129, 0.2139494, 0.49090424, 0.28710717, 0.72771484, 0.035503842, 0.8813849, 0.49935257, 0.26536158, 0.14064264, 0.6370699, 0.8249174, 0.046595335, 0.9307826, 0.862443, 0.026593428, 0.39963144, 0.5671936, 0.8786712, 0.28157595, 0.44560513, 0.3256588, 0.9856321, 0.49806264, 0.44184628, 0.39566737, 0.4946123, 0.7841833, 0.8711419, 0.14391732, 0.11152851, 0.16554162, 0.9964011, 0.305328, 0.113393284, 0.3941288, 0.8721979, 0.027368132, 0.80724365, 0.27576232, 0.44835758, 0.8118314, 0.6648749, 0.35443088, 0.39495277, 0.5290696, 0.99151295, 0.87658036, 0.64707667, 0.42810768, 0.9780855, 0.4976064, 0.37229943, 0.81678534, 0.25028443, 0.93633914, 0.058092706, 0.9686295, 0.4694417, 0.88084424, 0.40443346, 0.1290488, 0.047770884, 0.050411005, 0.718683, 0.42130944, 0.77474207, 0.3276638, 0.10004727, 0.36236504, 0.55885166, 0.55309576, 0.6978489, 0.35088632, 0.16761076, 0.7319193, 0.15657769, 0.70614874, 0.36026564, 0.8585169, 0.78698987, 0.09589582, 0.5240566, 0.97389257, 0.1252683, 0.9349652, 0.035219975, 0.112049095, 0.116087675, 0.29722258, 0.57299954, 0.8647291, 0.5110445, 0.036163967, 0.21864146, 0.94230807, 0.1444535, 0.38273305, 0.9427899, 0.26426572, 0.38839114, 0.8862351, 0.10936288, 0.5851739, 0.911402, 0.4038902, 0.31925252, 0.79429966, 0.5954674, 0.56786203]
        
        
        input = torch.FloatTensor([[input]])
        
        num_tests = 10000;
        
        start_time = time.time()
        for i in range(num_tests):
          output, state = self.forward(input, state);
        end_time = time.time()

        print("{} iterations in {} seconds".format(num_tests, end_time-start_time))
        #in_arr = input.data.numpy()[0][0]
        #print("matrix_val_t input[] = {");
        #for i in range(len(in_arr)):
          #print(in_arr[i], end=", ");
        #print("};");
        
        #out_arr = output.data.numpy()[0][0]
        #print("matrix_val_t output[] = {");
        #for i in range(len(out_arr)):
        #  print(out_arr[i], end=", ");
        #print("};");
        #print(input.data.numpy());
        #print(output.data.numpy());
        

    #-------------------------------------------------------------------------------------------
    def export(self):
        folder = os.path.join(self.session_directory, "matrices")
        if not os.path.exists(folder):
          os.makedirs(folder)
        
        layer_1_weights = self.layer_1.weight.data.numpy()
        np.save(os.path.join(folder, "layer_1_weights.npy"), layer_1_weights)
        
        layer_1_biases = self.layer_1.bias.data.numpy();
        np.save(os.path.join(folder, "layer_1_biases.npy"), layer_1_biases)

        #(W_ii|W_if|W_ig|W_io)
        W = np.vsplit(self.lstm.weight_ih_l0.data.numpy(), 4)
        np.save(os.path.join(folder, "Wi.npy"), W[0])
        np.save(os.path.join(folder, "Wf.npy"), W[1])
        np.save(os.path.join(folder, "Wg.npy"), W[2])
        np.save(os.path.join(folder, "Wo.npy"), W[3])

        U = np.vsplit(self.lstm.weight_hh_l0.data.numpy(), 4)
        np.save(os.path.join(folder, "Ui.npy"), U[0])
        np.save(os.path.join(folder, "Uf.npy"), U[1])
        np.save(os.path.join(folder, "Ug.npy"), U[2])
        np.save(os.path.join(folder, "Uo.npy"), U[3])
        
        b = np.hsplit(self.lstm.bias_ih_l0.data.numpy() + self.lstm.bias_hh_l0.data.numpy(), 4)
        np.save(os.path.join(folder, "bi.npy"), b[0])
        np.save(os.path.join(folder, "bf.npy"), b[1])
        np.save(os.path.join(folder, "bg.npy"), b[2])
        np.save(os.path.join(folder, "bo.npy"), b[3])

        layer_2_weights = self.layer_2.weight.data.numpy()
        np.save(os.path.join(folder, "layer_3_weights.npy"), layer_2_weights)
        
        layer_2_biases = self.layer_2.bias.data.numpy()
        np.save(os.path.join(folder, "layer_3_biases.npy"), layer_2_biases)
  
    #-------------------------------------------------------------------------------------------
    def sample_softmax_with_temperature(self, x, temperature) :
        return x.argmax().item()
        #x = x / temperature
        #p = torch.nn.functional.softmax(x, dim=0).detach().numpy()
        #return np.random.choice(len(x), p=p)
        
    #-------------------------------------------------------------------------------------------
    def reverse_synthesize_gold_standard(self, filename) :
        gold_standards = glob.glob(os.path.join(self.data_directory, "Validation/Gold_Standard/", filename + ".wav"))
        
        if len(gold_standards) < 1:
            print("Unable to find file {} for reverse synthesis".format(filename))
            return
        gold_standard = gold_standards[0]
        wav, sr = librosa.load(gold_standard, sr=self.sample_rate, mono=True)
        midi = MidiFile()
        track = MidiTrack()
        midi.tracks.append(track)
        start_sample = 0
        prev_notes = []
        output = torch.zeros(self.output_size) #np.zeros(self.output_size)
        #smoothed_spectrum = np.zeros(self.input_size)
        #smoothing_coefficient = 0.99
        frames_since_last_event = 0
        
        on_for = 1.0
        off_for = 1.0
        on_count = np.zeros(self.output_size)
        
        self.eval();
        
        state = self.get_initial_state(1)
        
        counter = 0;
        prev_active_note = -1;
        
        while (start_sample + self.input_size) < len(wav):
        
            input = wav[start_sample : start_sample + self.input_size]
            
            #input = [[self.calculate_audio_features(input)]]
            #input = [self.calculate_audio_features(input)]
            input = self.calculate_audio_features(input)
            
            #autoregressive input
            prev_output_vector, active_note = self.output_notes_to_vector(prev_notes)
            
            counter = counter + 1
            if(active_note != prev_active_note):
              counter = 0;
            prev_active_note = active_note
            
            input = [[np.concatenate((input, prev_output_vector, [counter]))]]
            
            #input = [np.concatenate((input, output.detach().numpy()))]
            
            #input = np.multiply(input, 2);
            
            #input = np.multiply(input, 1.0-smoothing_coefficient);
            #smoothed_spectrum = np.multiply(input, smoothing_coefficient);
            #input = np.add(input, smoothed_spectrum)
            
            current_notes = []
            
            input = torch.FloatTensor(input)
            
            output, state = self(input, state)
                        
            output = output[0][0]; #remove superfluous dimensions
            
            #output = output * 100000;
            #print(output)

            #max_output_index = output.argmax().item();
            #not really the max, just a sample with temperature
            temperature = 0.5
            max_output_index = self.sample_softmax_with_temperature(output, temperature);
            
            for i in range(len(output)) :
                #if np.random.sample() < output[i] :
                if i == max_output_index :
                    on_count[i] += 1.0 / on_for
                    if on_count[i] > 1 :
                        on_count[i] = 1
                    if (on_count[i] == 1) or (i + self.lowest_midi_note in prev_notes) :
                        current_notes.append(i + self.lowest_midi_note)
            
                else :
                    on_count[i] -= 1.0 / off_for
                    if on_count[i] <= 0 :
                        on_count[i] = 0
                    elif (i + self.lowest_midi_note in prev_notes) :
                        current_notes.append(i + self.lowest_midi_note)
            
            
            #for i in range(len(output)) :
            #    if output[i] > 0.5 :
            #    if np.random.sample() < output[i] :
            #        current_notes.append(i + self.lowest_midi_note)

            note_ons = np.setdiff1d(current_notes, prev_notes, assume_unique=True)
            note_offs = np.setdiff1d(prev_notes, current_notes, assume_unique=True)

            t = round((2 * midi.ticks_per_beat * frames_since_last_event * self.hop_size / sr))
  
            for n in note_offs:
                track.append(Message('note_off', note=n, velocity=0, time=t))
                t=0

            for n in note_ons:
                track.append(Message('note_on', note=n, velocity=64, time=t))
                t=0

            if len(note_offs) + len(note_ons) == 0:
              frames_since_last_event += 1
            else:
              frames_since_last_event = 1

            prev_notes = current_notes
            start_sample += self.hop_size
            
        #turn off any remaining notes at end of file
        for n in current_notes:
          track.append(Message('note_off', note=n, velocity=0, time=t))
        
        path = os.path.join(self.session_directory, self.model_save_prefix + "gold_standard_" + filename + str(self.get_saved_num_batches()).zfill(6) + ".mid")
        midi.save(path)
